---
sidebar_position: 0
---

# Module 4: Vision-Language-Action (VLA)

Welcome to the fourth and final module of The Autonomous Humanoid curriculum. This module focuses on implementing Vision-Language-Action models for humanoid robots.

## What You'll Learn

In this module, you'll explore:
- Integrating OpenAI Whisper for audio processing
- Cognitive planning with Large Language Models
- Completing the capstone project integrating all concepts

## Prerequisites

Before starting this module, ensure you have:
- Completed Modules 1, 2, and 3
- Understanding of AI and machine learning concepts

## Getting Started

Begin with the first chapter on [Whisper Integration](./whisper-integration.md) to learn about audio processing for humanoid robots.