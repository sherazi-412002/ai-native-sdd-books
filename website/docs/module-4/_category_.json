{
  "label": "Module 4: Vision-Language-Action (VLA)",
  "position": 4,
  "link": {
    "type": "generated-index",
    "description": "Implementing Vision-Language-Action models for humanoid robots. This module covers integrating OpenAI Whisper for audio processing, cognitive planning with large language models, and culminates in a comprehensive capstone project."
  }
}